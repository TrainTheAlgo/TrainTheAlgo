<script>
const article = {
    title: "How Generative AI Is Dismantling Our Civic Institutions",
    slug: "how-generative-ai-is-dismantling-our-civic-institutions",
    description: "Generative AI isn't just automating tasks. It is reshaping how institutions earn trust, make decisions, and stay accountable. Here's how the technology can hollow out democracy's support systems, and what to do before the damage hardens.",
    category: "AI",
    image: "how-generative-ai-is-dismantling-our-civic-institutions.png",
    research: "xAI Grok",
    author: "OpenAI ChatGPT",
    illustrator: "OpenAI ImageGen"
}
</script>
<style>
  .prose p { @apply text-slate-800 leading-relaxed; }
  .prose h2 { @apply text-slate-900 font-semibold mt-10 mb-3 text-2xl; }
  .prose h3 { @apply text-slate-900 font-semibold mt-6 mb-2 text-xl; }
  .prose a { @apply text-blue-700 underline decoration-blue-300 underline-offset-2; }
  .prose strong { @apply text-slate-900; }
  .callout { @apply bg-slate-50 border border-slate-200 rounded-xl p-5 my-6; }
  .quote { @apply bg-white border-l-4 border-slate-900/80 pl-4 py-2 my-6; }
</style>

<div class="prose max-w-3xl">
  <p class="text-lg">
    What if the most dangerous thing about generative AI is not that it can lie, but that it can make lying feel <em>normal</em>? Not as a scandal, not as a rare breach, but as the default texture of everyday life. If that sounds dramatic, it is worth reading on, because the real story is quieter and more unsettling: generative AI can erode the basic conditions that let civic institutions function at all, especially trust, accountability, and shared reality.
  </p>

  <p>
    That is the core warning in a recent preprint by Boston University law professors <strong>Woodrow Hartzog</strong> and <strong>Jessica Silbey</strong>, titled <em>How AI Destroys Institutions</em>. Their argument is not that every possible form of AI must be harmful. It is that the <strong>current design and incentives</strong> around predictive and generative systems, deployed at scale, tend to weaken institutions until they become brittle, performative, and easy to capture.
  </p>

  <div class="quote">
    <p>
      They describe AI as a tool that can "hollow out" institutions in governance, rule of law, education, healthcare, journalism, and even families, often under the banner of "optimization."
    </p>
  </div>

  <h2>What "destroying society" actually looks like</h2>
  <p>
    "Society" does not collapse only through riots or coups. It can also degrade through a slow loss of confidence in the systems that coordinate daily life. Courts work because people accept rulings even when they lose. Public health works because people believe guidance is grounded in evidence. Journalism works because audiences can tell reporting from propaganda. Schools work because credentials mean something beyond a PDF.
  </p>

  <p>
    Generative AI pressures all of these systems in the same direction. It makes it cheaper to produce convincing outputs than to produce careful ones. It makes it easier to simulate expertise than to earn it. And it makes it tempting for leaders to replace institutional judgment with automated "answers" that cannot be meaningfully interrogated.
  </p>

  <h2>The hidden mechanism: synthetic output beats human process</h2>
  <p>
    Institutions are not just outcomes. They are <strong>processes</strong>. They rely on deliberation, documentation, contestability, and the ability to appeal. Generative AI, by contrast, is built to produce fluent results quickly, often without a stable chain of reasoning that can be audited in human terms.
  </p>

  <p>
    This mismatch matters. When an institution adopts generative AI to speed up work, it often imports a new operating logic: produce more, faster, with less friction. Over time, the institution starts to look efficient on paper while becoming less legible, less accountable, and less trusted in practice.
  </p>

  <h2>Five ways generative AI accelerates institutional collapse</h2>

  <h3>1) It floods the public sphere with "cheap speech"</h3>
  <p>
    Democracies depend on a public sphere where attention is scarce and credibility is earned. Generative AI changes the economics. It becomes trivial to generate endless articles, comments, reviews, complaint letters, campaign ads, and "grassroots" messages. The result is not just misinformation. It is <strong>information inflation</strong>.
  </p>

  <p>
    When everything can be produced at scale, the signal-to-noise ratio collapses. People stop trying to verify. Journalists spend more time debunking than reporting. Platforms respond with automated moderation that makes mistakes at scale. Citizens disengage, not because they are lazy, but because the environment becomes cognitively unaffordable.
  </p>

  <div class="callout">
    <p class="font-semibold text-slate-900 mb-1">A practical test</p>
    <p>
      If your community cannot agree on what happened yesterday, it will struggle to agree on what to do tomorrow.
    </p>
  </div>

  <h3>2) It turns trust into a performance, not a relationship</h3>
  <p>
    Institutions earn legitimacy through consistent behavior over time. Generative AI can mimic the surface cues of legitimacy instantly. A polished memo. A confident medical explanation. A legal-sounding letter. A teacher-like response. The problem is that these cues can be produced without the underlying responsibility.
  </p>

  <p>
    This is where deepfakes and voice cloning are only the headline. The deeper shift is that <strong>authenticity becomes harder to prove than fakery is to produce</strong>. In that world, trust becomes a branding exercise. Whoever can generate the most convincing narrative wins the moment, even if they lose the truth.
  </p>

  <h3>3) It degrades accountability by obscuring who decided what</h3>
  <p>
    When a human official makes a decision, you can ask for reasons, documents, and standards. When a generative system drafts the rationale, suggests the policy, or summarizes the evidence, responsibility blurs. The institution can claim the model "helped," the vendor can claim the institution "chose," and the public is left with a fog of plausible deniability.
  </p>

  <p>
    This is not a theoretical risk. Many organizations already use AI to draft communications, screen applicants, triage cases, and summarize records. Even when humans remain "in the loop," the loop can become ceremonial. People defer to outputs that look authoritative, especially under time pressure.
  </p>

  <h3>4) It weaponizes efficiency against due process</h3>
  <p>
    Efficiency is not neutral in civic life. Courts, regulators, hospitals, and schools are intentionally slower than a chatbot because they must be fair, consistent, and reviewable. Generative AI is often sold as a way to remove "friction." But friction is frequently where rights live.
  </p>

  <p>
    Consider what happens when an agency uses AI to draft enforcement letters, or a school uses AI to evaluate student work, or a hospital uses AI to generate discharge instructions. The institution may process more cases, but individuals may find it harder to challenge errors, because the reasoning is generic, the sources are unclear, and the staff are trained to trust the system.
  </p>

  <h3>5) It centralizes power in the hands of model owners and data gatekeepers</h3>
  <p>
    Institutions are supposed to be durable and locally accountable. Generative AI, as currently deployed, often runs on centralized infrastructure controlled by a small number of companies. That creates dependency. Pricing changes, policy changes, model updates, and content rules can reshape how an institution operates without a vote, without a public hearing, and sometimes without notice.
  </p>

  <p>
    Even when models are open or self-hosted, the broader ecosystem still concentrates power through compute, proprietary data, and distribution. The risk is not only corporate influence. It is that public functions begin to inherit private incentives, quietly, through the tools they rely on.
  </p>

  <h2>Why this hits democracy harder than past technologies</h2>
  <p>
    Every major communication technology has been abused. Printing presses spread propaganda. Radio amplified demagogues. Social media optimized outrage. Generative AI adds a new capability: it can produce <strong>plausible, tailored content at industrial scale</strong>, and it can do so interactively, adapting to the target in real time.
  </p>

  <p>
    That combination matters for politics, but it also matters for everyday governance. A city council meeting can be swarmed with AI-generated submissions. A regulator can be buried in synthetic comments. A newsroom can be overwhelmed by fabricated tips and documents. A school can face a wave of indistinguishable assignments. The institution does not fail because it is corrupt. It fails because it cannot keep up with the volume of manufactured reality.
  </p>

  <h2>The "optimization" trap: when institutions start acting like apps</h2>
  <p>
    Hartzog and Silbey's framing lands because it describes a familiar pattern. Leaders adopt AI to cut costs and speed up outputs. Then they restructure around the tool. Staff are reduced. Expertise is devalued. Metrics shift from quality to throughput. The institution becomes more like a product team chasing engagement than a civic body pursuing legitimacy.
  </p>

  <p>
    Over time, the institution's public-facing work becomes smoother while its internal capacity to reason, explain, and correct itself gets thinner. That is how hollowing out works. It is not a single scandal. It is a gradual replacement of judgment with generation.
  </p>

  <h2>What to do if you run an institution, or rely on one</h2>
  <p>
    The most useful response is not panic. It is clarity about what must never be outsourced. Generative AI can be helpful for drafting, translation, and summarization, but only when the institution preserves the parts that create legitimacy: traceable evidence, accountable decision-makers, and meaningful appeal.
  </p>

  <p>
    Start by asking a blunt question in every deployment: <strong>if this output harms someone, who can explain it, fix it, and be held responsible?</strong> If the honest answer is "no one," the tool is not an efficiency upgrade. It is a liability generator.
  </p>

  <p>
    Next, treat provenance as a first-class requirement. Watermarking and detection are imperfect, but institutions can still demand documentation of sources, maintain human-authored records for key decisions, and separate "assistive drafts" from official reasoning. The goal is not to ban AI. It is to prevent the institution from losing its memory and its spine.
  </p>

  <p>
    Finally, protect the human roles that look expensive but are actually load-bearing. Investigators, editors, auditors, ombuds, public defenders, compliance officers, and experienced administrators are the people who turn messy reality into accountable action. If generative AI is used to replace them rather than support them, the institution may save money this quarter and spend legitimacy for the next decade.
  </p>

  <h2>The uncomfortable question underneath all of this</h2>
  <p>
    Generative AI is often framed as a productivity revolution. The harder framing is that it is a <strong>legitimacy stress test</strong>. It forces societies to decide what they value more: speed or due process, scale or accountability, convenience or trust.
  </p>

  <p>
    Because once a public institution learns to speak in perfectly polished sentences it did not truly write, the next thing it may forget is how to tell the difference between looking right and being right.
  </p>
</div>